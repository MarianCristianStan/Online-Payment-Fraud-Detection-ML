# -*- coding: utf-8 -*-
"""DT vs xGBoost vs RF

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QqRZF-z84tJM_0kcIoNpEwo1FMKX8rvt
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from xgboost import XGBClassifier, plot_tree
from imblearn.ensemble import BalancedRandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split,learning_curve
from imblearn.combine import SMOTETomek
import numpy as np
import time
import warnings

# Suppress warnings
warnings.filterwarnings("ignore")

data = pd.read_csv('onlinefraud.csv')
data = data.drop(columns=['step', 'nameOrig', 'nameDest', 'isFlaggedFraud'])

le = LabelEncoder()
data['type'] = le.fit_transform(data['type'])

X = data.drop(columns=['isFraud'])
y = data['isFraud']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, stratify=y, random_state=0)

# Apply SMOTETomek for balancing the training dataset
smote_tomek = SMOTETomek(random_state=0)
X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)
X_test_resampled, y_test_resampled = smote_tomek.fit_resample(X_test, y_test)

scale_pos_weight = len(y_train_resampled[y_train_resampled == 0]) / len(y_train_resampled[y_train_resampled == 1])

# Initialize models
xgb_model = XGBClassifier(
    max_depth=4,
    learning_rate=0.1,
    n_estimators=50,
    min_child_weight=10,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=scale_pos_weight,
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=0
)
rf_model = BalancedRandomForestClassifier(
    n_estimators=50,
    max_depth=4,
    min_samples_split=10,
    min_samples_leaf=10,
    max_features='sqrt',
    random_state=0
)
dt_model = DecisionTreeClassifier(
    max_depth=4,
    min_samples_split=10,
    min_samples_leaf=10,
    max_features='sqrt',
    random_state=0

)

# Train and time XGBoost
start = time.time()
xgb_model.fit(X_train_resampled, y_train_resampled)
xgb_train_time = time.time() - start
start = time.time()
y_pred_xgb = xgb_model.predict(X_test_resampled)
xgb_predict_time = time.time() - start

# Train and time Random Forest
start = time.time()
rf_model.fit(X_train_resampled, y_train_resampled)
rf_train_time = time.time() - start
start = time.time()
y_pred_rf = rf_model.predict(X_test_resampled)
rf_predict_time = time.time() - start

# Train and time Decision Tree
start = time.time()
dt_model.fit(X_train_resampled, y_train_resampled)
dt_train_time = time.time() - start
start = time.time()
y_pred_dt = dt_model.predict(X_test_resampled)
dt_predict_time = time.time() - start

# Evaluate models
models = {
    "XGBoost": (y_pred_xgb, xgb_train_time, xgb_predict_time),
    "Random Forest": (y_pred_rf, rf_train_time, rf_predict_time),
    "Decision Tree": (y_pred_dt, dt_train_time, dt_predict_time)
}

for name, (y_pred, train_time, predict_time) in models.items():
    print(f"\n{name} Results:")
    print(f"Training Time: {train_time:.2f} seconds")
    print(f"Prediction Time: {predict_time:.2f} seconds")
    print(f"Accuracy: {accuracy_score(y_test_resampled, y_pred):.3f}")

    # Confusion matrix
    conf_matrix = confusion_matrix(y_test_resampled, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='YlGn',
                xticklabels=['Legitimate', 'Fraudulent'],
                yticklabels=['Legitimate', 'Fraudulent'])
    plt.title(f'Confusion Matrix for {name}')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

# Compare Training and Prediction Times
import matplotlib.pyplot as plt

train_times = [xgb_train_time, rf_train_time, dt_train_time]
predict_times = [xgb_predict_time, rf_predict_time, dt_predict_time]
labels = ['XGBoost', 'Random Forest', 'Decision Tree']

plt.figure(figsize=(10, 5))
plt.bar(labels, train_times, color='blue', alpha=0.7)
plt.title('Training Time Comparison')
plt.ylabel('Time (seconds)')
plt.show()

plt.figure(figsize=(10, 5))
plt.bar(labels, predict_times, color='green', alpha=0.7)
plt.title('Prediction Time Comparison')
plt.ylabel('Time (seconds)')
plt.show()

plt.figure(figsize=(15, 5))

# XGBoost Feature Importance
plt.subplot(1, 3, 1)
sns.barplot(x=xgb_model.feature_importances_, y=X.columns)
plt.title('XGBoost Feature Importance')

# Random Forest Feature Importance
plt.subplot(1, 3, 2)
sns.barplot(x=rf_model.feature_importances_, y=X.columns)
plt.title('Random Forest Feature Importance')

# Decision Tree Feature Importance
plt.subplot(1, 3, 3)
sns.barplot(x=dt_model.feature_importances_, y=X.columns)
plt.title('Decision Tree Feature Importance')

plt.tight_layout()
plt.show()